{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c32241ec73104826b1eacf6314d3f707","deepnote_cell_type":"text-cell-h1"},"source":"# Training Deep Neural Networks","block_group":"5bfbabd36f934b6488413f9da3d7cc87"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d5268e8df7044126876b29f094f57086","deepnote_cell_type":"text-cell-h3"},"source":"### Training a DNN on CIFAR10 image dataset","block_group":"a4d7cc891edf4cefb55ecb72e00584a5"},{"cell_type":"code","metadata":{"source_hash":"799d5f6a","execution_start":1741764653291,"execution_millis":92811,"execution_context_id":"20a87514-fa90-4923-8fb5-0d4d4abef8bd","cell_id":"bc2a969722fd4c17a3b872621f64500c","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nimport math\n\n# Class for one cycle scheduler\nclass OneCycleScheduler(keras.callbacks.Callback):\n    def __init__(self, iterations, max_rate, start_rate=None, last_iterations=None, last_rate=None):\n        self.iterations = iterations\n        self.max_rate = max_rate\n        self.start_rate = start_rate or max_rate / 10\n        self.last_iterations = last_iterations or iterations // 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) // 2\n        self.last_rate = last_rate or self.start_rate / 1000\n        self.iteration = 0\n\n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return (rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1\n    \n    def on_batch_begin(self, batch, logs):\n        if self.iteration < self.half_iteration:\n            lr = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n        elif self.iterations < 2 * self.half_iteration:\n            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate)\n        else:\n            lr = self._interpolate(self.last_iterations, self.iterations, self.start_rate, self.last_rate)\n        self.iteration += 1\n        self.model.optimizer.learning_rate = lr\n\n# def build_model(hp):\n#     n_hidden = 20\n#     n_neurons = 100\n\n#     # Build model\n#     model = keras.Sequential()\n#     model.add(layers.Flatten(input_shape=(32, 32, 3)))\n#     for _ in range(n_hidden):\n#         model.add(layers.BatchNormalization())\n#         model.add(layers.Dense(n_neurons, activation=\"swish\", kernel_initializer=\"he_normal\"))\n#     model.add(layers.BatchNormalization())\n#     model.add(layers.Dense(10, activation=\"softmax\"))\n    \n#     optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n#     model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n#     return model\n\ndef build_model(hp):\n    n_hidden = 20\n    n_neurons = 100\n\n    # Build model\n    model = keras.Sequential()\n    model.add(layers.Flatten(input_shape=(32, 32, 3)))\n    # Standardize the data\n    # model.add(layers.Normalization())\n    # Self-normalizing layers\n    for _ in range(n_hidden):\n        model.add(layers.Dense(n_neurons, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n    model.add(layers.AlphaDropout(rate=0.1))\n    model.add(layers.Dense(10, activation=\"softmax\"))\n    \n    optimizer = keras.optimizers.SGD()\n    model.compile(\n        loss=\"sparse_categorical_crossentropy\",\n        optimizer=optimizer,\n        metrics=[\"accuracy\"]\n    )\n    return model\n\n# Set random seed\ntf.keras.utils.set_random_seed(42)\n\n# Load data set\n(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\nX_train = X_train_full[5000:]\ny_train = y_train_full[5000:]\nX_valid = X_train_full[:5000]\ny_valid = y_train_full[:5000]\n\nX_means = X_train.mean(axis=0)\nX_stds = X_train.std(axis=0)\nX_train_scaled = (X_train - X_means) / X_stds\nX_valid_scaled = (X_valid - X_means) / X_stds\nX_test_scaled = (X_test - X_means) / X_stds\n\n# Train the model\nbatch_size = 128\nn_epochs = 15\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\nmodel_chackpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.keras\", save_best_only=True)\nonecycle = OneCycleScheduler(\n    math.ceil(len(X_train_scaled) / batch_size) * n_epochs,\n    max_rate=0.05,\n)\nmodel = build_model(None)\nmodel.fit(\n    X_train_scaled,\n    y_train,\n    epochs=n_epochs, \n    validation_data=(X_valid_scaled, y_valid), \n    batch_size=batch_size,\n    callbacks=[early_stopping_cb, model_chackpoint_cb, onecycle]\n)\n\nmse_test, rmse_test = model.evaluate(X_test_scaled, y_test)\n","block_group":"bc2a969722fd4c17a3b872621f64500c","execution_count":2,"outputs":[{"name":"stderr","text":"2025-03-12 07:30:53.672939: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-12 07:30:53.678837: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-03-12 07:30:53.709032: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-12 07:30:53.709096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-12 07:30:53.709973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-12 07:30:53.714835: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-03-12 07:30:53.715324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-03-12 07:30:54.984395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nEpoch 1/15\n352/352 [==============================] - 7s 16ms/step - loss: 2.0668 - accuracy: 0.2838 - val_loss: 1.7492 - val_accuracy: 0.3712\nEpoch 2/15\n352/352 [==============================] - 5s 14ms/step - loss: 1.7734 - accuracy: 0.3738 - val_loss: 1.6987 - val_accuracy: 0.3978\nEpoch 3/15\n352/352 [==============================] - 5s 15ms/step - loss: 1.6347 - accuracy: 0.4205 - val_loss: 1.6856 - val_accuracy: 0.4046\nEpoch 4/15\n352/352 [==============================] - 5s 14ms/step - loss: 1.5579 - accuracy: 0.4456 - val_loss: 1.6345 - val_accuracy: 0.4246\nEpoch 5/15\n352/352 [==============================] - 6s 16ms/step - loss: 1.5041 - accuracy: 0.4667 - val_loss: 1.5920 - val_accuracy: 0.4508\nEpoch 6/15\n352/352 [==============================] - 5s 15ms/step - loss: 1.4627 - accuracy: 0.4784 - val_loss: 1.5946 - val_accuracy: 0.4372\nEpoch 7/15\n352/352 [==============================] - 6s 17ms/step - loss: 1.4031 - accuracy: 0.5026 - val_loss: 1.4573 - val_accuracy: 0.4950\nEpoch 8/15\n352/352 [==============================] - 5s 15ms/step - loss: 1.2292 - accuracy: 0.5631 - val_loss: 1.4579 - val_accuracy: 0.5014\nEpoch 9/15\n352/352 [==============================] - 6s 16ms/step - loss: 1.1990 - accuracy: 0.5726 - val_loss: 1.4575 - val_accuracy: 0.5070\nEpoch 10/15\n352/352 [==============================] - 6s 17ms/step - loss: 1.1820 - accuracy: 0.5782 - val_loss: 1.4568 - val_accuracy: 0.5068\nEpoch 11/15\n352/352 [==============================] - 6s 17ms/step - loss: 1.1684 - accuracy: 0.5826 - val_loss: 1.4563 - val_accuracy: 0.5078\nEpoch 12/15\n352/352 [==============================] - 5s 16ms/step - loss: 1.1587 - accuracy: 0.5851 - val_loss: 1.4598 - val_accuracy: 0.5112\nEpoch 13/15\n352/352 [==============================] - 5s 15ms/step - loss: 1.1485 - accuracy: 0.5881 - val_loss: 1.4632 - val_accuracy: 0.5136\nEpoch 14/15\n352/352 [==============================] - 5s 14ms/step - loss: 1.1420 - accuracy: 0.5923 - val_loss: 1.4639 - val_accuracy: 0.5120\nEpoch 15/15\n352/352 [==============================] - 5s 14ms/step - loss: 1.1379 - accuracy: 0.5917 - val_loss: 1.4641 - val_accuracy: 0.5126\n313/313 [==============================] - 1s 4ms/step - loss: 1.4759 - accuracy: 0.5098\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/48af3285-c158-436e-9729-e36ef294b96c","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=968d3c27-50e7-4d42-bdd9-442f6904c1c2' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"c3b71bf90f6844d3b88c085afc9eb076"}}