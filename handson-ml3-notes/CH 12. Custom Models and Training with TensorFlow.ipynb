{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1b01d9be552d4a64972c39178d15bd08","deepnote_cell_type":"text-cell-h1"},"source":"#  Custom Models and Training with TensorFlow","block_group":"24b75a89382d40b69ee9ed71a7cd3878"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"efc7f1638dca4dca832c902085b4515f","deepnote_cell_type":"text-cell-h3"},"source":"### Tensorflow Playground","block_group":"becf3376d023482f93bb0c2e347f6ae0"},{"cell_type":"code","metadata":{"source_hash":"1e0ff7a1","execution_start":1742367711282,"execution_millis":2568,"execution_context_id":"d1fa31b8-abbd-45ad-aff3-7a9c948dc8b2","cell_id":"ca96af1188354210a6490416ea8bc92f","deepnote_cell_type":"code"},"source":"import tensorflow as tf\n\n# t1 = tf.constant([\n#     [1.,2.,3.],\n#     [4.,5.,6.]\n# ])\n\n# print(t1.shape)\n# print(t1.dtype)\n\n# print(tf.math.log(t1))\n# print(tf.square(t1))\n\n# t2 = tf.constant([\n#     [7,8,9],\n#     [10,11,12]\n# ])\n\n# t1 + tf.cast(t2, dtype=tf.float32)\n\n# v = tf.Variable([\n#     [1.,2.,3.],\n#     [4.,5.,6.]\n# ])\n\n# v.assign([\n#     [7,8,9],\n#     [10,11,12]\n# ])\n\n# v.assign(v * 2)\n\nstr = \"Sample String\"\n\ntf_string = tf.convert_to_tensor(str)\n\ntf_string\n\nunicode_form = tf.strings.unicode_decode(tf_string, \"UTF-8\")\n\ntf.strings.unicode_encode(unicode_form, \"UTF-8\")","block_group":"8c7aceff6afa4021bc075124641a4c2e","execution_count":1,"outputs":[{"name":"stderr","text":"2025-03-19 07:01:51.577928: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-03-19 07:01:51.607534: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-19 07:01:51.607663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-19 07:01:51.608705: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-19 07:01:51.613984: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-03-19 07:01:51.614913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-03-19 07:01:52.619292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","output_type":"stream"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"<tf.Tensor: shape=(), dtype=string, numpy=b'Sample String'>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/b24910ad-80df-4fd1-ad6f-d2723109240c","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"36ac00dc7f574deebf901c3a38441a86","deepnote_cell_type":"text-cell-h3"},"source":"### Custom Cost Functions","block_group":"8c9b7de9e7f14f1081fd7a1f25d459bc"},{"cell_type":"code","metadata":{"source_hash":"aca498a9","execution_start":1742261198427,"execution_millis":0,"execution_context_id":"9007f62f-845d-495e-a022-5395729a729c","cell_id":"4c7f5e3645c140d389c1e4953e1aaefe","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nfrom tensorflow import keras\n\n# def huber_fn(y_true, y_pred):\n#     error = y_true - y_pred\n#     is_small_error = tf.abs(error) < 1\n#     squared_loss = tf.square(error) / 2\n#     linear_loss = tf.abs(error) - 0.5\n#     return tf.where(is_small_error, squared_loss, linear_loss)\n\nclass HuberLoss(keras.losses.Loss):\n    def __init__(self, threshold=1.0, **kwargs):\n        self.threshold = threshold\n        super().__init__(**kwargs)\n    def call(self, y_true, y_pred):\n        error = y_true - y_pred\n        is_small_error = tf.abs(error) < self.threshold\n        squared_loss = tf.square(error) / 2\n        linear_loss = tf.abs(error) - 0.5\n        return tf.where(is_small_error, squared_loss, linear_loss)\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}","block_group":"02b71e54986f4f02894cf761de00de10","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"fce49081cd03439381efe99e5e2803f1","deepnote_cell_type":"text-cell-h3"},"source":"### Custom Metrics","block_group":"874c0647a6974198bd95ee074ef5e11a"},{"cell_type":"code","metadata":{"source_hash":"8b8c4a06","execution_start":1742261198471,"execution_millis":0,"execution_context_id":"9007f62f-845d-495e-a022-5395729a729c","cell_id":"f6ad8eec83844479a18bbbb15cc52666","deepnote_cell_type":"code"},"source":"import tensorflow as tf\n\ndef create_huber(threshold=1.0):\n    def huber_fn(y_true, y_pred):\n        error = y_true - y_pred\n        is_small_error = tf.abs(error) < threshold\n        squared_loss = tf.square(error) / 2\n        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n        return tf.where(is_small_error, squared_loss, linear_loss)\n    return huber_fn\n\nclass HuberMetric(tf.keras.metrics.Metric):\n    def __init__(self, threshold=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.threshold = threshold\n        self.huber_fn = create_huber(threshold)\n        self.total = self.add_weight(\"total\", initializer = \"zeros\")\n        self.count = self.add_weight(\"count\", initializer = \"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        sample_metrics = self.huber_fn(y_true, y_pred)\n        self.total.assign_add(tf.reduce_sum(sample_metrics))\n        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n\n    def result(self):\n        return self.total / self.count\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}","block_group":"4210a355a2a6462daa7a913b322aa2e8","execution_count":3,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f42f272b3dcc48e8a103100f852cd0e6","deepnote_cell_type":"text-cell-h3"},"source":"### Custom Layers","block_group":"c0dd5b0431334cb59959f560ebaa507b"},{"cell_type":"code","metadata":{"source_hash":"d0c38f69","execution_start":1742261198519,"execution_millis":0,"execution_context_id":"9007f62f-845d-495e-a022-5395729a729c","cell_id":"c12d10dd463847df8b06b79e4f2f5101","deepnote_cell_type":"code"},"source":"import tensorflow as tf\n\nclass MyDense(tf.keras.layers.Layer):\n    def __init__(self, units, activation=None, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = tf.keras.activations.get(activation)\n    \n    def build(self, batch_input_shape):\n        self.kernel = slef.add_weight(\n            name=\"kernel\",\n            shape=[batch_input_shape[-1], self.units],\n            initializer=\"glorot_normal\"\n        )\n        self.bias = self.add_weight(\n            name=\"bias\",\n            shape=[self.units],\n            initializer=\"zeros\"\n        )\n    \n    def call(self, X):\n        return self.activation(X @ self.kernel + self.bias)\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {\n            **base_config, \n            \"units\": self.units, \n            \"activation\": tf.keras.activations.serialize(self.activation)\n        }\n","block_group":"10eedfa203094d1892561701a8379a4f","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"cf1526b5860e4dbf9f6191c2d8a8ce41","deepnote_cell_type":"text-cell-h3"},"source":"### Custon Model","block_group":"82c78e741cf14fde87d666c3c99740af"},{"cell_type":"code","metadata":{"source_hash":"38578a41","execution_start":1742261198571,"execution_millis":1,"execution_context_id":"9007f62f-845d-495e-a022-5395729a729c","cell_id":"831c5af05c1b40dcaf0a33d1456102e7","deepnote_cell_type":"code"},"source":"import tensorflow as tf\n\nclass ResidualBlock(tf.keras.layers.Layer):\n    def __init__(self, n_layers, n_neurons, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [\n            tf.keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\"\n            )\n            for _ in range(n_layers)\n        ]\n    \n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        return inputs + Z\n\nclass ResidualRegressor(tf.keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n        self.block1 = ResidualBlock(2, 30)\n        self.block2 = ResidualBlock(2, 30)\n        self.out = tf.keras.layers.Dense(output_dim)\n    \n    def call(self, inputs):\n        Z = self.hidden1(inputs)\n        for _ in range(1 + 3):\n            Z = self.block1(Z)\n        Z = self.block2(Z)\n        return self.out(Z)\n\nResidualRegressor(5).layers","block_group":"90e16c6cd5e74078b2934771bdf3c4e0","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"[<keras.src.layers.core.dense.Dense at 0x7f512ef61420>,\n <__main__.ResidualBlock at 0x7f512ef62920>,\n <__main__.ResidualBlock at 0x7f512ef61120>,\n <keras.src.layers.core.dense.Dense at 0x7f512ef63a00>]"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/3e2f5104-4140-453f-9daa-41f7603d9976","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"6c958282741542f5804baf9b86db7dd8","deepnote_cell_type":"text-cell-h3"},"source":"### Custom Losses and Metrics Based on Model Internals","block_group":"d264a4b370c24522a2c16ae22ee33ae7"},{"cell_type":"code","metadata":{"source_hash":"4819a01b","execution_start":1742261198619,"execution_millis":0,"execution_context_id":"9007f62f-845d-495e-a022-5395729a729c","cell_id":"a353ccbfa9a74c5e817d2136a7f2a005","deepnote_cell_type":"code"},"source":"import tensorflow as tf\n\nclass RecontructingRegressor(tf.keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super.__init__(**kwargs)\n        self.hidden = [\n            tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n            for _ in range(5)\n        ]\n        self.out = tf.keras.layers.Dense(output_dim)\n        self.reconstruction_mean = tf.keras.metrics.Mean(\n            name=\"reconstruction_error\"\n        )\n    \n    def build(self, batch_input_shape):\n        n_inputs = batch_input_shape[-1]\n        self.reconstrucy = tf.keras.layers.Dense(n_inputs)\n    \n    def call(self, inputs, training=False):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        reconstruction = self.reconstruct(Z)\n        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n        self.add_loss(0.05 * recon_loss)\n        if training:\n            result = self.reconstruction_mean(recon_loss)\n            self.add_metric(result)\n        return self.out(Z)\n","block_group":"34376d4500a0472c9de0706746959575","execution_count":6,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4df910db00ab4e2a8f5e928c304d23fc","deepnote_cell_type":"text-cell-h3"},"source":"### Custom Training Loop","block_group":"356624a3412648b2832513876a5b44af"},{"cell_type":"code","metadata":{"source_hash":"57eb5b69","execution_start":1742261198671,"execution_millis":24440,"execution_context_id":"9007f62f-845d-495e-a022-5395729a729c","cell_id":"344e65aa51a84e67ad6bfae5d0c176ff","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\n\ndef random_batch(X, y, batch_size=32):\n    idx = np.random.randint(len(X), size=batch_size)\n    return X[idx], y[idx]\n\ndef print_status_bar(step, total, loss, metrics=None):\n    metrics = \" - \".join(\n        [f\"{m.name}: {m.result():.4f}\" \n            for m in [loss] + (metrics or [])\n        ]\n    )\n    end = \"\" if step < total else \"\\n\"\n    print(f\"\\r{step}/{total} - \" + metrics, end=end)\n\n# Just for demonstration purposes\nX_train = np.random.rand(6400, 3)\ny_train = X_train[:, 0] + 2 * X_train[:, 1] + 3 * X_train[:, 2] + np.random.normal(0, 1, size=6400)\n\nl2_reg = tf.keras.regularizers.l2(0.05)\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(\n        30, \n        activation=\"relu\", \n        kernel_initializer=\"he_normal\",\n        kernel_regularizer=l2_reg\n    ),\n    tf.keras.layers.Dense(1, kernel_regularizer=l2_reg)\n])\n\nn_epochs = 15\nbatch_size = 32\nn_steps = len(X_train) // batch_size\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\nloss_fn = tf.keras.losses.mean_squared_error\nmean_loss = tf.keras.metrics.Mean()\nmetrics = [tf.keras.metrics.MeanAbsoluteError()]\n\nfor epoch in range(1, n_epochs + 1):\n    print(f\"Epoch {epoch}/{n_epochs}\")\n    for step in range(1, n_steps + 1):\n        X_batch, y_batch = random_batch(X_train, y_train)\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        mean_loss(loss)\n        for metric in metrics:\n            metric(y_batch, y_pred)\n        print_status_bar(step, n_steps, mean_loss, metrics)\n    for metric in [mean_loss] + metrics:\n        metric.reset_states()","block_group":"c107307203244267b58145b4380f7c56","execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/15\n200/200 - mean: 7.4803 - mean_absolute_error: 1.5324\nEpoch 2/15\n200/200 - mean: 5.2902 - mean_absolute_error: 0.9413\nEpoch 3/15\n200/200 - mean: 5.1353 - mean_absolute_error: 0.9583\nEpoch 4/15\n200/200 - mean: 5.0838 - mean_absolute_error: 0.9807\nEpoch 5/15\n200/200 - mean: 4.8872 - mean_absolute_error: 0.9815\nEpoch 6/15\n200/200 - mean: 4.8255 - mean_absolute_error: 0.9999\nEpoch 7/15\n200/200 - mean: 4.6561 - mean_absolute_error: 0.9957\nEpoch 8/15\n200/200 - mean: 4.5005 - mean_absolute_error: 0.9929\nEpoch 9/15\n200/200 - mean: 4.3998 - mean_absolute_error: 1.0009\nEpoch 10/15\n200/200 - mean: 4.3759 - mean_absolute_error: 1.0266\nEpoch 11/15\n200/200 - mean: 4.2764 - mean_absolute_error: 1.0336\nEpoch 12/15\n200/200 - mean: 4.1726 - mean_absolute_error: 1.0291\nEpoch 13/15\n200/200 - mean: 4.0593 - mean_absolute_error: 1.0259\nEpoch 14/15\n200/200 - mean: 3.9736 - mean_absolute_error: 1.0356\nEpoch 15/15\n200/200 - mean: 3.9738 - mean_absolute_error: 1.0679\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/2087d05a-883d-466c-be5a-b8101660ef8a","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"0cee525d89a14c56aae57c05c9df4e7b","deepnote_cell_type":"text-cell-h3"},"source":"### TensorFlow Functions and Graphs","block_group":"f54cb840cf67448f9a4778ad9305ab24"},{"cell_type":"code","metadata":{"source_hash":"ab4206b8","execution_start":1742261223163,"execution_millis":168,"execution_context_id":"9007f62f-845d-495e-a022-5395729a729c","cell_id":"f915714ada2e4868a972639d438a43c6","deepnote_cell_type":"code"},"source":"import tensorflow as tf\n\n@tf.function\ndef tf_cube(x):\n    print(100+100)\n    print(x)\n    return x ** 3\n\nprint(tf_cube.python_function)\n\nconcrete_function = tf_cube.get_concrete_function(tf.constant(2.0))\n\nops = concrete_function.graph.get_operations()\n\nresult = tf_cube(tf.constant(2.0))\nresult = tf_cube(tf.constant(3.0))\n\nresult = tf_cube(tf.constant([2.0, 3.0]))\n\nresult = tf_cube(2)\nresult = tf_cube(3)\n\n@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\ndef shrink(images):\n    return images[:, ::2, ::2]\n\nimg_batch_1 = tf.random.uniform(shape=[100,28,28])\nimg_batch_2 = tf.random.uniform(shape=[50,28,28])\n\npreprocessed_images = shrink(img_batch_1)\npreprocessed_images = shrink(img_batch_2)\n\n# TypeError: Binding inputs to tf.function failed\n# img_batch_3 = tf.random.uniform(shape=[2,2,2])\n# preprocessed_images = shrink(img_batch_3)\n\nprint(preprocessed_images.shape)\n\n# @tf.function\n# def add_10(x):\n#     for i in range(10):\n#         x += 1\n#     return x\n\n# add_10(tf.constant(0))\n\n# add_10.get_concrete_function(tf.constant(0)).graph.get_operations()\n\n@tf.function\ndef tf_add_10(x):\n    for i in tf.range(10):\n        x += 1\n    return x\n\ntf_add_10(tf.constant(0))\n\ntf_add_10.get_concrete_function(tf.constant(0)).graph.get_operations()\n","block_group":"80e25f55b06e4a9eb4eb0b869e9dcd37","execution_count":8,"outputs":[{"name":"stdout","text":"<function tf_cube at 0x7f512c4b7880>\n200\nTensor(\"x:0\", shape=(), dtype=float32)\n200\nTensor(\"x:0\", shape=(2,), dtype=float32)\n200\n2\n200\n3\n(50, 14, 14)\n","output_type":"stream"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"[<tf.Operation 'x' type=Placeholder>,\n <tf.Operation 'range/start' type=Const>,\n <tf.Operation 'range/limit' type=Const>,\n <tf.Operation 'range/delta' type=Const>,\n <tf.Operation 'range' type=Range>,\n <tf.Operation 'sub' type=Sub>,\n <tf.Operation 'floordiv' type=FloorDiv>,\n <tf.Operation 'mod' type=FloorMod>,\n <tf.Operation 'zeros_like' type=Const>,\n <tf.Operation 'NotEqual' type=NotEqual>,\n <tf.Operation 'Cast' type=Cast>,\n <tf.Operation 'add' type=AddV2>,\n <tf.Operation 'zeros_like_1' type=Const>,\n <tf.Operation 'Maximum' type=Maximum>,\n <tf.Operation 'while/maximum_iterations' type=Const>,\n <tf.Operation 'while/loop_counter' type=Const>,\n <tf.Operation 'while' type=StatelessWhile>,\n <tf.Operation 'Identity' type=Identity>]"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/02917293-4de0-458d-8d08-8c4af2288810","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9ac5bfb78bbc4a178fdf8e36844da418","deepnote_cell_type":"text-cell-h3"},"source":"### Implement a custom layer that performs layer normalization","block_group":"1cd1d75973514972a849260ad93c346e"},{"cell_type":"code","metadata":{"source_hash":"f8774412","execution_start":1742262566121,"execution_millis":50,"execution_context_id":"9007f62f-845d-495e-a022-5395729a729c","cell_id":"53a48b04f7224ec58824bf40946bd380","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nclass ElliotsLayer(tf.keras.layers.Layer):\n    def __init__(self, epsilon=0.001, **kwargs):\n        super().__init__(**kwargs)\n        self.epsilon = epsilon\n    \n    def build(self, batch_input_shape):\n        self.alpha = self.add_weight(\n            name=\"alpha\",\n            shape=batch_input_shape[-1:],\n            initializer=\"ones\",\n            dtype=\"float32\",\n        )\n        self.beta = self.add_weight(\n            name=\"beta\",\n            shape=batch_input_shape[-1:],\n            initializer=\"zeros\",\n            dtype=\"float32\",\n        )\n    \n    def call(self, X):\n        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n        return self.alpha * (X - mean) / tf.math.sqrt(variance + self.epsilon) + self.beta\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"epsilon\": self.epsilon}\n\nhousing = fetch_california_housing()\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    housing.data, housing.target.reshape(-1, 1), random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train_full, y_train_full, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)\n\nX = X_train.astype(np.float32)\ncustom_layer_norm = ElliotsLayer()\nkeras_layer_norm = tf.keras.layers.LayerNormalization()\n\n# error = tf.reduce_mean(\n#     tf.keras.losses.MeanAbsoluteError()(\n#         keras_layer_norm(X),\n#         custom_layer_norm(X)\n#     )\n# )\n# print(error)\n\n# Apply the custom layer once to initialize weights\ncustom_layer_norm.build(X.shape)\nkeras_layer_norm.build(X.shape)\n\ntf.keras.utils.set_random_seed(42)\nrandom_alpha = np.random.rand(X.shape[-1])\nrandom_beta = np.random.rand(X.shape[-1])\n\nkeras_layer_norm.set_weights([random_alpha, random_beta])\ncustom_layer_norm.set_weights([random_alpha, random_beta])\n\nrandom_error = tf.reduce_mean(\n    tf.keras.losses.MeanAbsoluteError()(\n        keras_layer_norm(X),\n        custom_layer_norm(X)\n    )\n)\n\nprint(random_error)","block_group":"ec8b3d3d1b68433fb0be692d2fb6e191","execution_count":36,"outputs":[{"name":"stdout","text":"tf.Tensor(0.66280156, shape=(), dtype=float32)\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/ba8c6871-c25f-4079-a0c5-2a75351703c8","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"64c5e339d87a402ea4506954baa16e09","deepnote_cell_type":"text-cell-h3"},"source":"### Train a model using a custom training loop to tackle the Fashion MNIST dataset","block_group":"5d35752da766478c96ee13bcd367b27e"},{"cell_type":"code","metadata":{"source_hash":"621f9d95","execution_start":1742276145865,"execution_millis":258456,"execution_context_id":"decfe5e9-4b5d-47ba-b843-a1abf542aaec","cell_id":"98d53704bcbc48feb415869bd57ba448","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nimport numpy as np\nfrom collections import OrderedDict\n\ndef random_batch(X, y, batch_size=32):\n    idx = np.random.randint(len(X), size=batch_size)\n    return X[idx], y[idx]\n\ndef print_status_bar(step, total, loss, metrics=None):\n    metrics = \" - \".join(\n        [f\"{m.name}: {m.result():.4f}\" \n            for m in [loss] + (metrics or [])\n        ]\n    )\n    end = \"\" if step < total else \"\\n\"\n    print(f\"\\r{step}/{total} - \" + metrics, end=end)\n\nfashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\nX_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\nX_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n\n# Preprocessing of data\nX_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.\nclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\n# model = tf.keras.Sequential([\n#     tf.keras.layers.Flatten(input_shape=[28, 28]),\n#     tf.keras.layers.Dense(300, activation=\"relu\"),\n#     tf.keras.layers.Dense(100, activation=\"relu\"),\n#     tf.keras.layers.Dense(10, activation=\"softmax\")\n# ])\n\n# n_epoch = 5\n# batch_size = 32\n# n_steps = len(X_train) // 32\n# optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n# loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n# mean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n# metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n\n# for epoch in range(1, n_epoch + 1):\n#     print(f\"Epoch {epoch}/{n_epoch}\")\n#     for step in range(1, n_steps + 1):\n#         X_batch, y_batch = random_batch(X_train, y_train)\n#         with tf.GradientTape() as tape:\n#             y_pred = model(X_batch)\n#             main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n#             loss = tf.add_n([main_loss] + model.losses)\n#         gradients = tape.gradient(loss, model.trainable_variables)\n#         optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n#         status = OrderedDict()\n#         mean_loss(loss)\n#         status[\"loss\"] = mean_loss.result().numpy()\n#         for metric in metrics:\n#             metric(y_batch, y_pred)\n#             status[metric.name] = metric.result().numpy()\n#         print_status_bar(step, n_steps, mean_loss, metrics)\n    \n#     y_pred = model(X_valid)\n#     status[\"val_loss\"] = tf.reduce_mean(loss_fn(y_valid, y_pred))\n#     status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n#         tf.constant(y_valid, dtype=np.float32), y_pred\n#     ))\n\n#     for metric in [mean_loss] + metrics:\n#         metric.reset_states()\n\n# Split the model into two parts and apply a different optimizer to each part\nlower_layer = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\n    tf.keras.layers.Dense(300, activation=\"relu\"),\n])\nupper_layer = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel = tf.keras.Sequential([\n    lower_layer,\n    upper_layer\n])\nlower_optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\nupper_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)\n\nn_epoch = 5\nbatch_size = 32\nn_steps = len(X_train) // 32\nloss_fn = tf.keras.losses.sparse_categorical_crossentropy\nmean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\nmetrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n\nfor epoch in range(1, n_epoch + 1):\n    print(f\"Epoch {epoch}/{n_epoch}\")\n    for step in range(1, n_steps + 1):\n        X_batch, y_batch = random_batch(X_train, y_train)\n        with tf.GradientTape(persistent=True) as tape:\n            lower_output = lower_layer(X_batch)\n            y_pred = upper_layer(lower_output)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        # TODO\n        lower_grads = tape.gradient(loss, lower_layer.trainable_variables)\n        upper_grads = tape.gradient(loss, upper_layer.trainable_variables)\n        lower_optimizer.apply_gradients(zip(lower_grads, lower_layer.trainable_variables))\n        upper_optimizer.apply_gradients(zip(upper_grads, upper_layer.trainable_variables))\n        del tape\n        status = OrderedDict()\n        mean_loss(loss)\n        status[\"loss\"] = mean_loss.result().numpy()\n        for metric in metrics:\n            metric(y_batch, y_pred)\n            status[metric.name] = metric.result().numpy()\n        print_status_bar(step, n_steps, mean_loss, metrics)\n    \n    y_pred = model(X_valid)\n    status[\"val_loss\"] = tf.reduce_mean(loss_fn(y_valid, y_pred))\n    status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n        tf.constant(y_valid, dtype=np.float32), y_pred\n    ))\n\n    for metric in [mean_loss] + metrics:\n        metric.reset_states()\n    ","block_group":"0ca0c4226fcc4127bf6951dd418e25b3","execution_count":1,"outputs":[{"name":"stderr","text":"2025-03-18 05:35:46.275260: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-18 05:35:46.277924: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-03-18 05:35:46.309909: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-18 05:35:46.310010: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-18 05:35:46.310884: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-18 05:35:46.316575: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-03-18 05:35:46.319455: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-03-18 05:35:47.587400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\nEpoch 1/5\n1718/1718 - mean_loss: 0.5657 - sparse_categorical_accuracy: 0.8053\nEpoch 2/5\n1718/1718 - mean_loss: 0.4333 - sparse_categorical_accuracy: 0.8447\nEpoch 3/5\n1718/1718 - mean_loss: 0.3873 - sparse_categorical_accuracy: 0.8605\nEpoch 4/5\n1718/1718 - mean_loss: 0.3689 - sparse_categorical_accuracy: 0.8661\nEpoch 5/5\n1718/1718 - mean_loss: 0.3472 - sparse_categorical_accuracy: 0.8716\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/8aaa2b34-9032-4f9d-894f-0faf9a4a0c3a","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=968d3c27-50e7-4d42-bdd9-442f6904c1c2' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2025-03-19T07:21:38.414Z"},"deepnote_notebook_id":"f5f9da7dfb5949a1963a844f10d6e061"}}