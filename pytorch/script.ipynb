{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"048911ca6d0e4cd3aee68c91581fe871","deepnote_cell_type":"text-cell-h2"},"source":"## Introduction to PyTorch","block_group":"9092976eb8984fe6919ddbcdd7cd4d9c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"acf3a812aeff407a9e83c42391a07313","deepnote_cell_type":"text-cell-h3"},"source":"### Installing PyTorch","block_group":"de312ace986b4cf9bfdb663c2f1231ed"},{"cell_type":"code","metadata":{"source_hash":"51bfe403","execution_start":1746668494536,"execution_millis":1898,"execution_context_id":"f33d0775-d722-4367-8e85-adf31d8e5143","cell_id":"98796b03d2d547f889bb57d7735d74da","deepnote_cell_type":"code"},"source":"import torch\n\ndef to_onehot(y, num_classes):\n    y_onehot = torch.zeros(y.size(0), num_classes)\n    y_onehot.scatter_(1, y.view(-1, 1).long(), 1).float()\n    return y_onehot\n\ny = torch.tensor([0, 1, 2, 2])\n\ny_enc = to_onehot(y, 3)\n\nprint('one-hot encoding:\\n', y_enc)","block_group":"98796b03d2d547f889bb57d7735d74da","execution_count":1,"outputs":[{"name":"stderr","text":"/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\none-hot encoding:\n tensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 1.]])\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/632d9d6d-4886-4c55-89dc-614068b5c60e","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"64e170981c8245098ae5b81af097cd56","deepnote_cell_type":"text-cell-h3"},"source":"### Understanding tensors","block_group":"1a53da960e9548be8dd26ba03dab9130"},{"cell_type":"code","metadata":{"source_hash":"33311d8f","execution_start":1746672908084,"execution_millis":2,"execution_context_id":"4a990654-f450-4013-9e28-66e3f7b3267a","cell_id":"340f9ab2f1de4218be37f32e981f72f0","deepnote_cell_type":"code"},"source":"import torch\n\ntensor0d = torch.tensor(1)\ntensor1d = torch.tensor([1, 2, 3])\ntensor2d = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\ntensor3d = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n\nprint(tensor3d)\nprint(tensor3d.shape)\n\nprint(tensor1d.dtype)\nprint(tensor2d.dtype)\nprint(tensor3d.to(torch.float32).dtype)\n\nprint(tensor2d.T)\nprint(tensor2d @ tensor2d.T)","block_group":"ae6302ae1a1d492ca02eb92a906caebf","execution_count":4,"outputs":[{"name":"stdout","text":"tensor([[[ 1,  2,  3],\n         [ 4,  5,  6]],\n\n        [[ 7,  8,  9],\n         [10, 11, 12]]])\ntorch.Size([2, 2, 3])\ntorch.int64\ntorch.float32\ntorch.float32\ntensor([[1., 4.],\n        [2., 5.],\n        [3., 6.]])\ntensor([[14., 32.],\n        [32., 77.]])\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/0d3074c8-115e-4f55-b615-3dea07a599a3","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f44a2cadfc2b4bf8ad767a828b07c4cb","deepnote_cell_type":"text-cell-h3"},"source":"### Computing gradients via autograd","block_group":"99bb0377b0994a1f8386ba50203bae95"},{"cell_type":"code","metadata":{"source_hash":"e869d103","execution_start":1746676358258,"execution_millis":1,"execution_context_id":"ee104998-48fd-4c71-9ad5-2c6d4d6bdbf5","cell_id":"6e4cbca7e8894b6d930d50a99bdbec26","deepnote_cell_type":"code"},"source":"import torch\nimport torch.nn.functional as F\nfrom torch.autograd import grad\n\ny = torch.tensor([1.0])\nx1 = torch.tensor([1.1])\nw1 = torch.tensor([2.2], requires_grad=True)\nb = torch.tensor([0.0], requires_grad=True)\n\nz = x1 * w1 + b\na = torch.sigmoid(z)\n\nloss = F.binary_cross_entropy(a, y)\n\n# grad_L_w1 = grad(loss, w1, retain_graph=True)\n# print(grad_L_w1)\n# grad_L_b = grad(loss, b, retain_graph=True)\n# print(grad_L_b)\n\nloss.backward()\nprint(w1.grad)\nprint(b.grad)","block_group":"99e5d95b50ee4949b92e953c12b5e29c","execution_count":13,"outputs":[{"name":"stdout","text":"tensor([-0.0898])\ntensor([-0.0817])\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/51b587ab-d17b-43ed-a38f-038fda451701","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"ad4fdfc4a6f24950beefffe39fe5102a","deepnote_cell_type":"text-cell-h3"},"source":"### Implementing multilayer neural networks","block_group":"2a708c4bff724f5283c56ffc0c2fe5d3"},{"cell_type":"code","metadata":{"source_hash":"d5b6e14","execution_start":1746683045456,"execution_millis":1,"execution_context_id":"dfa423e7-4c58-4f0d-80e4-6c33df619d17","cell_id":"4f8d91ddb8474dc6966670eeef30a91d","deepnote_cell_type":"code"},"source":"import torch\n\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self, num_inputs, num_outputs):\n        super().__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(num_inputs, 30),\n            torch.nn.ReLU(),\n            torch.nn.Linear(30, 20),\n            # torch.nn.ReLU(),\n            # torch.nn.Linear(20, 30),\n            # torch.nn.ReLU(),\n            # torch.nn.Linear(30, 20),\n            torch.nn.ReLU(),\n            torch.nn.Linear(20, num_outputs),\n        )\n\n    def forward(self, x):\n        logits = self.layers(x)\n        return logits\n\ntorch.manual_seed(42)\nmodel = NeuralNetwork(num_inputs=50, num_outputs=3)\nprint(model)\n\nX = torch.rand((1, 50))\nout = model(X)\nprint(out)","block_group":"c83d2a5832b94ece8c0547fef6b94143","execution_count":21,"outputs":[{"name":"stdout","text":"NeuralNetwork(\n  (layers): Sequential(\n    (0): Linear(in_features=50, out_features=30, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=30, out_features=20, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=20, out_features=30, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=30, out_features=20, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=20, out_features=3, bias=True)\n  )\n)\ntensor([[-0.0926,  0.0474,  0.0505]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/71306605-3aff-4caa-bd85-07c7afc01f7e","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d2e841b76cee4f9f9c111a0de0a32150","deepnote_cell_type":"text-cell-h3"},"source":"### Setting up efficient data loaders","block_group":"007054ce37934ffa9b3eed0f226b7712"},{"cell_type":"code","metadata":{"source_hash":"6e5d18d9","execution_start":1746689013228,"execution_millis":4,"execution_context_id":"77723d16-76cb-4fd7-9cb5-43b3ccbed48a","cell_id":"339152aa9cdf45379d98c07537adf6bb","deepnote_cell_type":"code"},"source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nX_train = torch.tensor([\n    [-1.2, 3.1],\n    [-0.9, 2.9],\n    [-0.5, 2.6],\n    [2.3, -1.1],\n    [2.7, -1.5]\n])\ny_train = torch.tensor([0, 0, 0, 1, 1])\n\nX_test = torch.tensor([\n    [-0.8, 2.8],\n    [2.6, -1.6]\n])\ny_test = torch.tensor([0, 1])\n\n\nclass ToyDataset(Dataset):\n    def __init__(self, X, y):\n        self.features = X\n        self.labels = y\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\ntrain_dataset = ToyDataset(X_train, y_train)\ntest_dataset = ToyDataset(X_test, y_test)\n\nprint(train_dataset[0])\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=0,\n    drop_last=True\n)\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=True,\n    num_workers=0,\n    drop_last=True\n)\n\n\nfor index, (X, y) in enumerate(train_data_loader):\n    print(X)\n    print(y)","block_group":"3859f7f5977b4cf4b4c5b530098107c2","execution_count":31,"outputs":[{"name":"stdout","text":"(tensor([-1.2000,  3.1000]), tensor(0))\ntensor([[ 2.7000, -1.5000],\n        [-0.9000,  2.9000]])\ntensor([1, 0])\ntensor([[-0.5000,  2.6000],\n        [ 2.3000, -1.1000]])\ntensor([0, 1])\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/742f08db-8d8b-4138-a602-73c4511c677e","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"40e3cba9d0f74802b2f45bc4938630c9","deepnote_cell_type":"text-cell-h3"},"source":"### A typical training loop","block_group":"0fbe701084454b0aa84a83f1887269a1"},{"cell_type":"code","metadata":{"source_hash":"80fe9a74","execution_start":1746695390647,"execution_millis":235,"execution_context_id":"2663bc59-066d-46fa-a6d5-f4f0a00eb13a","cell_id":"15a64f3e01b2403dacc424d068044a53","deepnote_cell_type":"code"},"source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self, num_inputs, num_outputs):\n        super().__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(num_inputs, 30),\n            torch.nn.ReLU(),\n            torch.nn.Linear(30, 20),\n            # torch.nn.ReLU(),\n            # torch.nn.Linear(20, 30),\n            # torch.nn.ReLU(),\n            # torch.nn.Linear(30, 20),\n            torch.nn.ReLU(),\n            torch.nn.Linear(20, num_outputs),\n        )\n\n    def forward(self, x):\n        logits = self.layers(x)\n        return logits\n\nclass ToyDataset(Dataset):\n    def __init__(self, X, y):\n        self.features = X\n        self.labels = y\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\ndef compute_accuracy(model, data_loader):\n    model.eval()\n    correct = 0.0\n    total_example = 0\n    for index, (features, labels) in enumerate(data_loader):\n        with torch.no_grad():\n            logits = model(features)\n        predictions = torch.argmax(logits, dim=1)\n        compare = predictions == labels\n        correct += torch.sum(compare)\n        total_example += len(compare)\n    return (correct/total_example).item()\n# Set random seed\ntorch.manual_seed(42)\n\n# Prepare the datasets\nX_train = torch.tensor([\n    [-1.2, 3.1],\n    [-0.9, 2.9],\n    [-0.5, 2.6],\n    [2.3, -1.1],\n    [2.7, -1.5]\n])\ny_train = torch.tensor([0, 0, 0, 1, 1])\n\nX_test = torch.tensor([\n    [-0.8, 2.8],\n    [2.6, -1.6]\n])\ny_test = torch.tensor([0, 1])\n\ntrain_dataset = ToyDataset(X_train, y_train)\ntest_dataset = ToyDataset(X_test, y_test)\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=0,\n    drop_last=True\n)\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=True,\n    num_workers=0,\n    drop_last=True\n)\n\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n\nmax_epoch = 3\n\nfor epoch in range(max_epoch):\n    model.train()\n    for batch_index, (features, labels) in enumerate(train_data_loader):\n        # Compute prediction\n        logits = model(features)\n        loss = F.cross_entropy(logits, labels)\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"Epoch: {epoch+1:03d}/{max_epoch:03d}\"\n              f\" | Batch: {batch_index+1:03d}/{len(train_data_loader):03d}\"\n              f\" | Loss: {loss:.4f}\"\n        )\nmodel.eval()\n\n# with torch.no_grad():\n#     outputs = model(X_test)\n# print(outputs)\n# torch.set_printoptions(sci_mode=False)\n# # probas = torch.softmax(outputs, dim=1)\n# # print(probas)\n# # predictions = torch.argmax(probas, dim=1)\n# # print(predictions)\n# predictions = torch.argmax(outputs, dim=1)\n# print(predictions)\n\nprint(compute_accuracy(model, train_data_loader))\nprint(compute_accuracy(model, test_data_loader))\n\ntorch.save(model.state_dict(), \"model.pth\")\n\nmodel = NeuralNetwork(2,2)\nmodel.load_state_dict(torch.load(\"model.pth\"))\n\nprint(compute_accuracy(model, train_data_loader))\nprint(compute_accuracy(model, test_data_loader))","block_group":"a495f27be69b4338a8a3ab67854a53ea","execution_count":19,"outputs":[{"name":"stdout","text":"Epoch: 001/003 | Batch: 001/002 | Loss: 0.6117\nEpoch: 001/003 | Batch: 002/002 | Loss: 0.3457\nEpoch: 002/003 | Batch: 001/002 | Loss: 0.2022\nEpoch: 002/003 | Batch: 002/002 | Loss: 0.0770\nEpoch: 003/003 | Batch: 001/002 | Loss: 0.0348\nEpoch: 003/003 | Batch: 002/002 | Loss: 0.0147\n1.0\n1.0\n1.0\n1.0\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/9a964d13-d34c-4816-9080-e07e7f5a1f1d","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=968d3c27-50e7-4d42-bdd9-442f6904c1c2' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2025-05-08T09:31:31.517Z"},"deepnote_notebook_id":"2c658ecbdb2d4aab95addd7d15deaf6a"}}